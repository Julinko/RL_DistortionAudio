{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning für Verzerrer-Parameteroptimierung\n",
    "\n",
    "In diesem Jupyter-Notebook verwenden wir Q-Learning, um die optimalen Parameter für einen Verzerrer zu finden. Der Code ist in verschiedene Sektionen unterteilt:\n",
    "\n",
    "**1. Einrichtung und Initialisierung**\n",
    "- **Bibliotheken**: Import von notwendigen Bibliotheken wie `numpy` und `scipy`.\n",
    "\n",
    "**2. DistortionParameters-Klasse**\n",
    "- **Zweck**: Repräsentiert die Parameter des Verzerrers: `Gain`, `Tone` und `Level`.\n",
    "- **Methoden**: Zum Setzen und Abrufen dieser Parameter.\n",
    "\n",
    "**3. Environment-Klasse: Verzerrer und Signal**\n",
    "- **Signal**: Generiert ein Sinussignal und verarbeitet es mit dem Verzerrer.\n",
    "- **Verarbeitung**: Methoden wie `_apply_gain`, `_apply_tone` und `_apply_level`.\n",
    "\n",
    "**4. Q-Learning-Agent-Klasse**\n",
    "- **Algorithmus**: Implementiert den Q-Learning-Algorithmus.\n",
    "- **Q-Tabelle**: Wird zum Lernen der besten Aktionen (Parameter) verwendet.\n",
    "- **Methoden**: Zum Wählen von Aktionen und Aktualisieren der Q-Tabelle.\n",
    "\n",
    "**5. Training**\n",
    "- **Prozess**: Trainiert den Q-Learning-Agenten über mehrere Episoden.\n",
    "- **Belohnung**: In jeder Episode wird das Signal verarbeitet und eine Belohnung basierend auf dem Unterschied zum Ziel-Ausgangssignal berechnet.\n",
    "\n",
    "**6. Ergebnisse**\n",
    "- **Ausgabe**: Zeigt die gelernten optimalen Parameter nach dem Training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sektion 1: Einrichtung und Initialisierung**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import butter, lfilter\n",
    "from numpy.fft import fft\n",
    "\n",
    "# Addons\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sektion 2: DistortionParameters-Klasse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistortionParameters:\n",
    "    def __init__(self, gain=0.5, tone=0.5, level=0.5):\n",
    "        self.gain = gain\n",
    "        self.tone = tone\n",
    "        self.level = level\n",
    "    \n",
    "    def set_parameters(self, gain, tone, level):\n",
    "        self.gain = gain\n",
    "        self.tone = tone\n",
    "        self.level = level\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.gain, self.tone, self.level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sektion 3: Environment-Klasse: Verzerrer und Signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistortionEnvironment:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.signal = self._generate_signal()\n",
    "        self.target_signal = self.process(self.signal)\n",
    "    \n",
    "    def _generate_signal(self):\n",
    "        t = np.linspace(0, 1, 44100)\n",
    "        return np.sin(2 * np.pi * 440 * t)\n",
    "    \n",
    "    def process(self, signal):\n",
    "        return self._apply_level(self._apply_tone(self._apply_gain(signal, self.params.gain), self.params.tone), self.params.level)\n",
    "    \n",
    "    def _apply_gain(self, signal, gain):\n",
    "        return np.clip(signal * gain, -1, 1)\n",
    "    \n",
    "    def _apply_tone(self, signal, tone):\n",
    "        nyq = 0.5 * 44100\n",
    "        low = 300\n",
    "        high = 6000\n",
    "        cutoff = low + (high - low) * tone\n",
    "        b, a = butter(1, cutoff / nyq, btype='low')\n",
    "        return lfilter(b, a, signal)\n",
    "    \n",
    "    def _apply_level(self, signal, level):\n",
    "        return signal * level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Q-Learning-Agent-Klasse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Parameter vll. mit CrossValidation? Bzw. Recherche was\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((11, 11, 11))\n",
    "    \n",
    "    def choose_action(self):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.rand(3)  # Random Action\n",
    "        else:\n",
    "            indices = np.unravel_index(np.argmax(self.q_table, axis=None), self.q_table.shape)  # Greedy Action\n",
    "            return indices[0] / 10, indices[1] / 10, indices[2] / 10\n",
    "    \n",
    "    def update(self, state, next_state, reward):\n",
    "        future_value = np.max(self.q_table[tuple((np.array(next_state) * 10).astype(int))])\n",
    "        index = tuple((np.array(state) * 10).astype(int))\n",
    "        self.q_table[index] = (\n",
    "            self.q_table[index] + \n",
    "            self.alpha * (reward + self.gamma * future_value - self.q_table[index])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "\n",
    "In diesem Abschnitt werden Hilfsfunktionen definiert, die im weiteren Verlauf des Notebooks verwendet werden:\n",
    "\n",
    "1. **`moving_average`**:\n",
    "    - **Zweck**: Berechnet den gleitenden Durchschnitt einer gegebenen Datenreihe.\n",
    "    - **Parameter**:\n",
    "        - `values`: Die Datenreihe.\n",
    "        - `window`: Die Größe des Fensters für den gleitenden Durchschnitt.\n",
    "    - **Rückgabewert**: Eine Liste der gleitenden Durchschnittswerte.\n",
    "</br></br>\n",
    "\n",
    "2. **`compute_snr`**:\n",
    "    - **Zweck**: Berechnet das Signal-Rausch-Verhältnis (SNR) zwischen einem Signal und einem Referenzsignal.\n",
    "    - **Parameter**:\n",
    "        - `signal`: Das zu bewertende Signal.\n",
    "        - `reference`: Das Referenzsignal.\n",
    "    - **Rückgabewert**: Das berechnete SNR.\n",
    "</br></br>\n",
    "\n",
    "3. **`reward_function`**:\n",
    "    - **Zweck**: Berechnet die Belohnung basierend auf dem Unterschied zwischen dem Ausgangssignal und dem Ziel-Audiosignal. Die Funktion berücksichtigt sowohl den Fehler im Frequenzbereich als auch das SNR.\n",
    "    - **Parameter**:\n",
    "        - `output_signal`: Das Ausgangssignal des Verzerrers.\n",
    "        - `target_signal`: Das Ziel-Audiosignal.\n",
    "    - **Rückgabewert**: Ein Wert, der die Belohnung (oder den negativen Fehler) repräsentiert.\n",
    "</br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    # Berechnet den gleitenden Durchschnitt\n",
    "    return np.convolve(values, np.ones(window)/window, mode='valid')\n",
    "\n",
    "def compute_snr(signal, reference):\n",
    "    error = signal - reference\n",
    "    signal_power = np.mean(signal**2)\n",
    "    error_power = np.mean(error**2)\n",
    "    \n",
    "    # Überprüfen, ob der Fehler null ist\n",
    "    if error_power == 0:\n",
    "        return float('inf')\n",
    "    return 10 * np.log10(signal_power / error_power)\n",
    "\n",
    "def reward_function(output_signal, target_signal):\n",
    "    # Transform both signals into the frequency domain\n",
    "    output_freq = fft(output_signal)\n",
    "    target_freq = fft(target_signal)\n",
    "    \n",
    "    # Calculate the error in the frequency domain\n",
    "    freq_error = np.abs(output_freq - target_freq)\n",
    "    \n",
    "    # Weight the error by frequency (for simplicity, this example uses linear weighting)\n",
    "    weighted_error = freq_error * np.linspace(1, 2, len(freq_error))\n",
    "    \n",
    "    # Compute the SNR\n",
    "    snr = compute_snr(output_signal, target_signal)\n",
    "    \n",
    "    # Combine the weighted error and SNR (you can adjust the weighting factor)\n",
    "    combined_error = np.mean(weighted_error) - 0.01 * snr\n",
    "    \n",
    "    # Überprüfen, ob der kombinierte Fehler gültig ist\n",
    "    if np.isnan(combined_error) or np.isinf(combined_error):\n",
    "        return -np.mean(weighted_error)  # Verwenden Sie nur den gewichteten Fehler als Belohnung\n",
    "    \n",
    "    return -combined_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sektion 5: Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized:\n",
      "gain : 0.5\n",
      "tone : 0.3\n",
      "level : 0.7\n",
      "Randomized:\n",
      "gain : 0.5\n",
      "tone : 0.3\n",
      "level : 0.7\n",
      "Episode 100/100000, Average Reward (last 100 episodes): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24362/419958568.py:11: RuntimeWarning: divide by zero encountered in log10\n",
      "  return 10 * np.log10(signal_power / error_power)\n",
      "/tmp/ipykernel_24362/3561349719.py:21: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  self.alpha * (reward + self.gamma * future_value - self.q_table[index])\n",
      "/home/orwell/Desktop/_repos/RL_DistortionAudio/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/orwell/Desktop/_repos/RL_DistortionAudio/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 1900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 2900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 3900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4300/100000, Average Reward (last 100 episodes): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24362/3561349719.py:20: RuntimeWarning: invalid value encountered in scalar add\n",
      "  self.q_table[index] +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 4900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 5900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 6900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 7900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 8900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 9900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 10900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 11900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 12900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 13900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 14900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 15900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 16900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 17900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 18900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 19900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 20900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 21900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 22900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 23900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 24900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 25900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 26900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 27900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28100/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28200/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28300/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28400/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28500/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28600/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28700/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28800/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 28900/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 29000/100000, Average Reward (last 100 episodes): nan\n",
      "Episode 29100/100000, Average Reward (last 100 episodes): nan\n"
     ]
    }
   ],
   "source": [
    "convergence_threshold = 0.01  # Minimale Änderung der durchschnittlichen Belohnung\n",
    "convergence_window = 5000  # Anzahl der Episoden, über die die Konvergenz überprüft wird\n",
    "epochs = 100000\n",
    "\n",
    "params = DistortionParameters(round(np.random.rand(), 1), round(np.random.rand(), 1), round(np.random.rand(), 1))\n",
    "env = DistortionEnvironment(params)\n",
    "agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "rewards = deque(maxlen=2*convergence_window)\n",
    "\n",
    "\n",
    "# Print random gains\n",
    "print(\"Randomized:\")\n",
    "for key, value in vars(params).items():\n",
    "    print(key, \":\", value)\n",
    "\n",
    "print_interval = 100  # Status alle 100 Episoden drucken\n",
    "print_indices = set(range(print_interval, epochs + 1, print_interval))\n",
    "convergence_indices = set(range(convergence_window, epochs + 1, convergence_window))\n",
    "\n",
    "\n",
    "\n",
    "# Print random gains\n",
    "print(\"Randomized:\")\n",
    "for key, value in vars(params).items():\n",
    "    print(key, \":\", value)\n",
    "\n",
    "print_indices = set(range(print_interval, epochs + 1, print_interval))\n",
    "convergence_indices = set(range(convergence_window, epochs + 1, convergence_window))\n",
    "\n",
    "# Prozess\n",
    "for episode in range(1, epochs + 1):  # Start episode count from 1\n",
    "    current_state = [params.gain, params.tone, params.level]\n",
    "    action = agent.choose_action()\n",
    "    params.set_parameters(*action)\n",
    "    output_signal = env.process(env.signal)\n",
    "    reward = reward_function(output_signal, env.target_signal)\n",
    "    next_state = [params.gain, params.tone, params.level]  # Zustand nach Anwendung der Aktion\n",
    "    agent.update(current_state, next_state, reward)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Status drucken\n",
    "    if episode in print_indices:\n",
    "        avg_reward_last_n = np.mean(list(rewards)[-print_interval:])\n",
    "        print(f\"Episode {episode}/{epochs}, Average Reward (last {print_interval} episodes): {avg_reward_last_n:.3f}\")\n",
    "\n",
    "    # Überprüfen Sie die Konvergenz\n",
    "    if episode in convergence_indices:\n",
    "        avg_reward_last_n = np.mean(list(rewards)[-convergence_window:])\n",
    "        avg_reward_prev_n = np.mean(list(rewards)[-2*convergence_window:-convergence_window])\n",
    "        \n",
    "        if abs(avg_reward_last_n - avg_reward_prev_n) < convergence_threshold:\n",
    "            print(f\"Convergence achieved at episode {episode}. Stopping training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sektion 6: Ergebnisse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echte Parameter - Gain: 0.1 Tone: 0.6 Level: 0.3\n",
      "Gelernte Parameter - Gain: 0.1 Tone: 0.6 Level: 0.3\n"
     ]
    }
   ],
   "source": [
    "optimal_indices = np.unravel_index(np.argmax(agent.q_table, axis=None), agent.q_table.shape)\n",
    "optimal_params = DistortionParameters(optimal_indices[0] / 10, optimal_indices[1] / 10, optimal_indices[2] / 10)\n",
    "\n",
    "print(\"Echte Parameter - Gain:\", params.gain, \"Tone:\", params.tone, \"Level:\", params.level)\n",
    "print(\"Gelernte Parameter - Gain:\", optimal_params.gain, \"Tone:\", optimal_params.tone, \"Level:\", optimal_params.level)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
